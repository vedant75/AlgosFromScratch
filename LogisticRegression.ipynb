{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUGzIYh6bOhekdre5EsEnu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedant75/AlgosFromScratch/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2qZGmHt_TJ5P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Optional, Callable, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _add_intercept(X: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Return X with a leading column of ones.\n",
        "    Input: X: (n, p) or (n,) -> output (n, p+1)\n",
        "    \"\"\"\n",
        "    X = np.asarray(X)\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    ones = np.ones((X.shape[0], 1), dtype=float)\n",
        "    return np.hstack([ones, X])\n",
        "\n",
        "class LogisticRegression:\n",
        "  def __init__(self, lr=0.01, n_iter=100_000, solver='batch', verbose=False, tol=1e-5, shuffle=True, batch_size=32):\n",
        "    self.bias =None\n",
        "    self.weight =None\n",
        "    self.lr =lr\n",
        "    self.n_iter = n_iter\n",
        "    self.cost_history = []\n",
        "    self.verbose = verbose\n",
        "    self.tol = tol\n",
        "    self.solver = solver\n",
        "    self.shuffle = shuffle\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(z):\n",
        "    z = np.asarray(z)\n",
        "    out = np.empty_like(z, dtype='float')\n",
        "    pos = z>=0\n",
        "    out[pos] = 1.0/(1.0 + np.exp(-z[pos]))\n",
        "    neg = ~pos\n",
        "    e = np.exp(z[neg])\n",
        "    out[neg] = e/(1+e)\n",
        "    return out\n",
        "\n",
        "  def _loss(self, a, y):\n",
        "    y = np.asarray(y)\n",
        "    m = y.shape[0]\n",
        "    epsilon = 1e-15\n",
        "    a = np.clip(a, epsilon, 1-epsilon)\n",
        "    loss = (-1/m)*np.sum(y*np.log(a) + (1-y)*np.log(1-a))\n",
        "    return loss\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    if self.solver == 'batch':\n",
        "      return self.batch_gd(X,y)\n",
        "    elif self.solver == 'sgd':\n",
        "      return self.sgd_gd(X,y)\n",
        "    elif self.solver == 'mini_batch_gd':\n",
        "      return self.mini_batch_gd(X,y)\n",
        "    elif self.solver == 'newton':\n",
        "      return self.newton(X,y)\n",
        "\n",
        "\n",
        "  def batch_gd(self, X, y):\n",
        "    X=np.asarray(X, dtype='float')\n",
        "    y=np.asarray(y, dtype='float')\n",
        "    assert X.shape[0] == y.shape[0]\n",
        "    m, n = X.shape\n",
        "    if self.weight is None:\n",
        "      self.weight = np.zeros(n, dtype='float')\n",
        "    self.bias = 0.0\n",
        "\n",
        "    prev_loss = 0.0\n",
        "    for epoch in range(self.n_iter):\n",
        "      # z = wx + b\n",
        "      z = np.dot(X, self.weight) + self.bias\n",
        "      a = LogisticRegression.sigmoid(z)\n",
        "\n",
        "      dw = np.dot(X.T, (a-y))/m\n",
        "      db = np.sum(a-y)/m\n",
        "\n",
        "      self.weight -= self.lr*dw\n",
        "      self.bias -= self.lr*db\n",
        "\n",
        "      cur_loss = self._loss(a, y)\n",
        "\n",
        "      # 2. Check for convergence for early stopping\n",
        "      if abs(cur_loss - prev_loss) < self.tol and epoch > 0:\n",
        "          print(f\"Converged at iteration {epoch}. Stopping early.\")\n",
        "          break # Exit the loop\n",
        "\n",
        "      prev_loss = cur_loss\n",
        "      self.cost_history.append(cur_loss)\n",
        "    return self\n",
        "\n",
        "  def sgd_gd(self, X, y):\n",
        "    X = np.asarray(X, dtype='float')\n",
        "    y = np.asarray(y, dtype='float')\n",
        "    assert X.shape[0] == y.shape[0]\n",
        "    m, n = X.shape\n",
        "    if self.weight is None:\n",
        "      self.weight = np.zeros(n, dtype='float')\n",
        "    if self.bias is None:\n",
        "      self.bias =0.0\n",
        "    prev_loss = float('inf')\n",
        "    for epoch in range(self.n_iter):\n",
        "      if self.shuffle:\n",
        "        indices = np.arange(m)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "      for i in indices:\n",
        "        # z[i] = w*X[i] + b\n",
        "        z = np.dot(X[i], self.weight) + self.bias\n",
        "        a = LogisticRegression.sigmoid(z)\n",
        "        error = (a-y[i])\n",
        "\n",
        "        dw = X[i]*error\n",
        "        db = error\n",
        "\n",
        "        self.weight -= self.lr*dw\n",
        "        self.bias -= self.lr*db\n",
        "\n",
        "      full_pred = LogisticRegression.sigmoid(np.dot(X, self.weight)+self.bias)\n",
        "      cur_loss = self._loss(full_pred, y)\n",
        "      if self.verbose and (epoch % max(1, self.n_iter//10) ==0):\n",
        "        print(f\"Epoch {epoch:6d}  Loss={cur_loss:.6f}\")\n",
        "      if abs(cur_loss - prev_loss) < self.tol and epoch>0:\n",
        "        print(f'Early stopping at epoch {epoch}')\n",
        "        break\n",
        "\n",
        "      prev_loss = cur_loss\n",
        "      self.cost_history.append(cur_loss)\n",
        "    return self\n",
        "\n",
        "  def mini_batch_gd(self, X, y):\n",
        "    X = np.asarray(X, dtype='float')\n",
        "    y = np.asarray(y, dtype='float')\n",
        "    assert X.shape[0] == y.shape[0]\n",
        "    m,n = X.shape\n",
        "    if self.weight is None:\n",
        "      self.weight = np.zeros(n, dtype='float')\n",
        "    if self.bias is None:\n",
        "      self.bias = 0.0\n",
        "\n",
        "    prev_loss = float('inf')\n",
        "    for epoch in range(self.n_iter):\n",
        "      if self.shuffle:\n",
        "        indices= np.arange(m)\n",
        "        np.random.shuffle(indices)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "      else:\n",
        "          X_shuffled = X\n",
        "          y_shuffled = y\n",
        "\n",
        "      for i in range(0, m, self.batch_size):\n",
        "        # z[0:batch_size] = X_shuffled[0:batch_size] + b\n",
        "        z = np.dot(X_shuffled[i:i+self.batch_size], self.weight) + self.bias\n",
        "        a = LogisticRegression.sigmoid(z)\n",
        "        error = (a-y_shuffled[i:i+self.batch_size])\n",
        "\n",
        "        dw = np.dot(X_shuffled[i:i+self.batch_size].T, error)/len(y_shuffled[i:i+self.batch_size])\n",
        "        db = np.sum(error)/len(y_shuffled[i:i+self.batch_size])\n",
        "\n",
        "        self.weight -= self.lr*dw\n",
        "        self.bias -= self.lr*db\n",
        "\n",
        "      full_pred = LogisticRegression.sigmoid(np.dot(X_shuffled, self.weight)+self.bias)\n",
        "      cur_loss = self._loss(full_pred, y)\n",
        "      if self.verbose and (epoch % max(1, self.n_iter//10)==0):\n",
        "         print(f\"Epoch {epoch:6d}  Loss={cur_loss:.6f}\")\n",
        "      if abs(cur_loss - prev_loss) < self.tol and epoch>0:\n",
        "        print(f'Early stopping at epoch {epoch}')\n",
        "        break\n",
        "\n",
        "      prev_loss = cur_loss\n",
        "      self.cost_history.append(cur_loss)\n",
        "    return self\n",
        "\n",
        "  def newton(self, X, y):\n",
        "    X = np.asarray(X, dtype='float')\n",
        "    y = np.asarray(y, dtype='float')\n",
        "    assert X.shape[0] == y.shape[0]\n",
        "    X = _add_intercept(X)\n",
        "    m,n = X.shape\n",
        "    if self.weight is None:\n",
        "      self.weight = np.zeros(n, dtype='float')\n",
        "    prev_loss = float('inf')\n",
        "    for epoch in range(self.n_iter):\n",
        "      z = np.dot(X, self.weight)\n",
        "      a = LogisticRegression.sigmoid(z)\n",
        "      grad = np.dot(X.T, (a-y))/m\n",
        "\n",
        "      W = a*(1-a)\n",
        "      X_b = W[:, None] * X\n",
        "      hessian = np.dot(X.T, X_b) / m\n",
        "\n",
        "      try:\n",
        "        step = np.linalg.solve(hessian + 1e-6 * np.eye(n), grad)\n",
        "        self.weight -= step\n",
        "      except np.linalg.LinAlgError:\n",
        "        # Fallback to gradient descent if Hessian is singular\n",
        "        self.weight -= self.lr * grad\n",
        "\n",
        "\n",
        "      cur_loss = self._loss(a, y)\n",
        "\n",
        "      # 2. Check for convergence for early stopping\n",
        "      if abs(cur_loss - prev_loss) < self.tol and epoch > 0:\n",
        "          print(f\"Converged at iteration {epoch}. Stopping early.\")\n",
        "          break # Exit the loop\n",
        "\n",
        "      prev_loss = cur_loss\n",
        "      self.cost_history.append(cur_loss)\n",
        "    return self\n",
        "\n",
        "  def predict_proba(self, X):\n",
        "    X = np.asarray(X)\n",
        "\n",
        "    # Check which logic to use based on the solver\n",
        "    if self.solver == 'newton':\n",
        "        # Newton's method uses an intercept, no separate bias\n",
        "        X_aug = _add_intercept(X)\n",
        "        z = np.dot(X_aug, self.weight)\n",
        "    else:\n",
        "        # Other solvers use a separate weight and bias\n",
        "        z = np.dot(X, self.weight) + self.bias\n",
        "\n",
        "    return self.sigmoid(z)\n",
        "\n",
        "  def predict(self, X, threshold=0.5):\n",
        "    y_prob = self.predict_proba(X)\n",
        "    return (y_prob >= threshold).astype(int)"
      ],
      "metadata": {
        "id": "48ofAGszTLdl"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}